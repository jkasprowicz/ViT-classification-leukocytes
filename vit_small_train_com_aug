import pandas as pd
import os
import json
from PIL import Image
from torch.utils.data import Dataset, DataLoader
import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import transforms
import timm
from tqdm import tqdm
import matplotlib.pyplot as plt
from torch.utils.data import WeightedRandomSampler
import torchmetrics
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

# Lê o arquivo CSV e obtém os nomes das classes
df = pd.read_csv('/lapix/train/_classes.csv')
class_names = sorted(df['label'].unique().tolist())
print(f"Nomes das classes encontrados: {class_names}")

# ----------------- Dataset -----------------
class LeukocyteDataset(Dataset):
    def __init__(self, csv_file, root_dir, transform=None):
        self.annotations = pd.read_csv(csv_file)
        self.root_dir = root_dir
        self.transform = transform
        self.label_names = sorted(self.annotations['label'].unique())
        self.label_to_idx = {label: idx for idx, label in enumerate(self.label_names)}

    def __len__(self):
        return len(self.annotations)

    def __getitem__(self, idx):
        img_name = os.path.join(self.root_dir, self.annotations.iloc[idx, 0])
        image = Image.open(img_name).convert('RGB')
        label_name = self.annotations.iloc[idx, 1]
        label = self.label_to_idx[label_name]

        if self.transform:
            image = self.transform(image)

        return image, label

# ----------------- Hiperparâmetros -----------------
BATCH_SIZE = 16
NUM_EPOCHS = 50
LR = 1e-5
patience = 7 # <-- SUGESTÃO: Aumentar a paciência, pois a regularização pode levar a uma convergência mais lenta

# ----------------- DataLoaders -----------------
train_transform = transforms.Compose([
    transforms.Resize((384, 384)),
    transforms.RandomHorizontalFlip(p=0.5),
    transforms.RandomVerticalFlip(p=0.5),
    transforms.RandomRotation(degrees=45),
    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.2), # <-- SUGESTÃO: Data Augmentation um pouco mais forte
    transforms.ToTensor(),
    transforms.Normalize([0.5]*3, [0.5]*3)
])

val_transform = transforms.Compose([
    transforms.Resize((384, 384)),
    transforms.ToTensor(),
    transforms.Normalize([0.5]*3, [0.5]*3)
])

# SEU CÓDIGO MODIFICADO
NOVO_CAMINHO_IMAGENS_TREINO = '/lapix/HistAuGAN/dataset/images'
train_dataset = LeukocyteDataset('/lapix/_classes.csv', NOVO_CAMINHO_IMAGENS_TREINO, transform=train_transform)
val_dataset = LeukocyteDataset('/lapix/valid/_classes.csv', 'valid', transform=val_transform)

# ----------------- Amostragem Ponderada (Weighted Sampler) -----------------
label_counts = train_dataset.annotations['label'].value_counts()
class_weights = 1.0 / label_counts
sample_weights = [class_weights[label] for label in train_dataset.annotations['label']]
sample_weights_tensor = torch.DoubleTensor(sample_weights)
sampler = WeightedRandomSampler(weights=sample_weights_tensor, num_samples=len(sample_weights_tensor), replacement=True)

train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, sampler=sampler, num_workers=4)
val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)

# ----------------- Dispositivo (Device) -----------------
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Usando dispositivo: {device}")

# ----------------- Modelo -----------------
# 1. MUDANÇA: Trocando para um modelo menor e adicionando mais dropout
model = timm.create_model(
    'vit_small_patch16_384', # <-- MUDANÇA AQUI
    pretrained=True,
    num_classes=len(train_dataset.label_names),
    drop_rate=0.3  # <-- MUDANÇA AQUI: Adiciona mais regularização de dropout
)
model.to(device)

criterion = nn.CrossEntropyLoss()

# 2. MUDANÇA: Usando o otimizador AdamW com weight_decay
optimizer = optim.AdamW(model.parameters(), lr=LR, weight_decay=1e-2) # <-- MUDANÇA AQUI

scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=15, gamma=0.1)

# --- O restante do seu código de treinamento, salvamento e plotagem permanece o mesmo ---
# (O código foi omitido por brevidade, mas ele continua igual ao que você já tinha)
with open('label_names.json', 'w') as f:
    json.dump(train_dataset.label_names, f)
print("✅ Nomes dos rótulos salvos para consistência em testes futuros.")

history = {
    'epoch': [], 'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': [],
    'val_precision': [], 'val_recall': [], 'val_f1': []
}
num_classes = len(train_dataset.label_names)
val_precision_metric = torchmetrics.classification.Precision(task="multiclass", num_classes=num_classes, average='macro').to(device)
val_recall_metric = torchmetrics.classification.Recall(task="multiclass", num_classes=num_classes, average='macro').to(device)
val_f1_metric = torchmetrics.classification.F1Score(task="multiclass", num_classes=num_classes, average='macro').to(device)
best_val_loss = float('inf')
epochs_no_improve = 0
all_preds_final = []
all_labels_final = []

for epoch in range(NUM_EPOCHS):
    model.train()
    train_loss, correct, total = 0, 0, 0
    for images, labels in tqdm(train_loader, desc=f"Época {epoch+1}/{NUM_EPOCHS} [Treino]"):
        images, labels = images.to(device), labels.to(device)
        optimizer.zero_grad()
        outputs = model(images)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        train_loss += loss.item() * images.size(0)
        _, predicted = outputs.max(1)
        correct += predicted.eq(labels).sum().item()
        total += labels.size(0)
    avg_train_loss = train_loss / total
    train_acc = correct / total

    model.eval()
    val_loss, correct, total = 0, 0, 0
    all_preds_epoch, all_labels_epoch = [], []
    with torch.no_grad():
        for images, labels in tqdm(val_loader, desc=f"Época {epoch+1}/{NUM_EPOCHS} [Val]"):
            images, labels = images.to(device), labels.to(device)
            outputs = model(images)
            loss = criterion(outputs, labels)
            val_loss += loss.item() * images.size(0)
            _, predicted = outputs.max(1)
            correct += predicted.eq(labels).sum().item()
            total += labels.size(0)
            val_precision_metric.update(predicted, labels)
            val_recall_metric.update(predicted, labels)
            val_f1_metric.update(predicted, labels)
            all_preds_epoch.extend(predicted.cpu().numpy())
            all_labels_epoch.extend(labels.cpu().numpy())
    avg_val_loss = val_loss / total
    val_acc = correct / total
    val_precision = val_precision_metric.compute()
    val_recall = val_recall_metric.compute()
    val_f1 = val_f1_metric.compute()
    val_precision_metric.reset()
    val_recall_metric.reset()
    val_f1_metric.reset()

    print(f"\n✅ Época [{epoch+1}/{NUM_EPOCHS}] Treino Loss: {avg_train_loss:.4f} | Treino Acc: {train_acc:.4f}\n"
          f"Val Loss: {avg_val_loss:.4f}  | Val Acc: {val_acc:.4f} | Val Precision: {val_precision:.4f} | Val Recall: {val_recall:.4f} | Val F1: {val_f1:.4f}\n")
    
    history['epoch'].append(epoch + 1); history['train_loss'].append(avg_train_loss); history['train_acc'].append(train_acc)
    history['val_loss'].append(avg_val_loss); history['val_acc'].append(val_acc); history['val_precision'].append(val_precision.item())
    history['val_recall'].append(val_recall.item()); history['val_f1'].append(val_f1.item())
    
    scheduler.step()

    if avg_val_loss < best_val_loss:
        best_val_loss = avg_val_loss
        epochs_no_improve = 0
        torch.save(model.state_dict(), "vit_best_model.pth")
        all_preds_final, all_labels_final = all_preds_epoch, all_labels_epoch
        print(f"✅ Loss de validação melhorou, modelo salvo como vit_best_model.pth")
    else:
        epochs_no_improve += 1
        print(f"⚠️ Sem melhora por {epochs_no_improve}/{patience} épocas.")
    if epochs_no_improve >= patience:
        print(f"⛔ Early stopping ativado na época {epoch+1}. Melhor Val Loss: {best_val_loss:.4f}")
        break
    
    torch.save(model.state_dict(), f"vit_epoch_{epoch+1}.pth")
    torch.cuda.empty_cache()

df_history = pd.DataFrame(history)
df_history.to_csv('training_metrics.csv', index=False)
print("✅ Métricas de treinamento salvas em training_metrics.csv")

# ... (código de plotagem e matriz de confusão) ...
# ----------------- Plotar Curvas de Métricas ----------------- # <-- Modificado
plt.figure(figsize=(20, 6))

# Loss
plt.subplot(1, 3, 1)
plt.plot(history['epoch'], history['train_loss'], label='Train Loss')
plt.plot(history['epoch'], history['val_loss'], label='Validation Loss')
plt.xlabel('Época'); plt.ylabel('Loss'); plt.title('Loss ao Longo das Épocas'); plt.legend(); plt.grid(True)

# Acurácia
plt.subplot(1, 3, 2)
plt.plot(history['epoch'], history['train_acc'], label='Train Accuracy')
plt.plot(history['epoch'], history['val_acc'], label='Validation Accuracy')
plt.xlabel('Época'); plt.ylabel('Acurácia'); plt.title('Acurácia ao Longo das Épocas'); plt.legend(); plt.grid(True)

# Precision, Recall, F1
plt.subplot(1, 3, 3)
plt.plot(history['epoch'], history['val_precision'], label='Validation Precision')
plt.plot(history['epoch'], history['val_recall'], label='Validation Recall')
plt.plot(history['epoch'], history['val_f1'], label='Validation F1-Score')
plt.xlabel('Época'); plt.ylabel('Score'); plt.title('Precision, Recall & F1-Score'); plt.legend(); plt.grid(True)

plt.tight_layout()
plt.savefig('training_curves_all_metrics.png', dpi=300)
plt.show()
print("✅ Curvas de métricas salvas em training_curves_all_metrics.png")

print("\n🎯 Pipeline de treinamento concluído com sucesso.")
