import json
import pandas as pd

# Caminho para o arquivo salvo
with open("vit-checkpoints/checkpoint-6384/trainer_state.json", "r") as f:
    trainer_state = json.load(f)

# Extrair log_history
log_history = trainer_state["log_history"]

# Converter em DataFrame
df = pd.DataFrame(log_history)
df.head()

import matplotlib.pyplot as plt

df_eval = df[df['eval_loss'].notna()]  # apenas pontos com avaliação

plt.plot(df_eval["epoch"], df_eval["eval_loss"], label="Eval Loss")
plt.plot(df_eval["epoch"], df_eval["eval_accuracy"], label="Eval Accuracy")
plt.xlabel("Epoch")
plt.legend()
plt.title("Evaluation Loss and Accuracy per Epoch")
plt.show()

from transformers import ViTForImageClassification, ViTImageProcessor

# Load best checkpoint model
model = ViTForImageClassification.from_pretrained("./vit-checkpoints/checkpoint-3591")
processor = ViTImageProcessor.from_pretrained("google/vit-base-patch16-224-in21k")

import evaluate

accuracy = evaluate.load("accuracy")

def compute_metrics(eval_pred):
    logits, labels = eval_pred
    preds = logits.argmax(-1)
    return accuracy.compute(predictions=preds, references=labels)
def transform(example):
    inputs = processor(images=example["image"], return_tensors="pt")
    pixel_values = inputs["pixel_values"].squeeze(0)
    return {
        "pixel_values": pixel_values,
        "labels": example["label"]
    }

train_ds = train_ds.with_transform(transform)
test_ds = test_ds.with_transform(transform)

print(test_ds.column_names)

from transformers import Trainer

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_ds,
    eval_dataset=test_ds,
    tokenizer=processor,
    compute_metrics=compute_metrics,
)

metrics = trainer.evaluate()
print(metrics)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

inputs = processor(images=img, return_tensors="pt")
inputs = {k: v.to(device) for k, v in inputs.items()}  # move input tensors to the device

with torch.no_grad():
    outputs = model(**inputs)



from transformers import ViTForImageClassification, ViTImageProcessor
import torch
from PIL import Image

# Path to your saved checkpoint folder, e.g. "vit-checkpoints/checkpoint-3591"
checkpoint_path = "vit-checkpoints/checkpoint-3591"

# Load processor and model from checkpoint folder
processor = ViTImageProcessor.from_pretrained(checkpoint_path)
model = ViTForImageClassification.from_pretrained(checkpoint_path)

# Put model in eval mode
model.eval()

label_names = ['Artefato', 'Basofilo', 'Bastonete', 'Blasto', 'Eosinofilo', 'Eritroblasto',
               'Linfocito', 'Linfocito atipico', 'Metamielocito', 'Mielocito', 'Monocito',
               'Neutrofilo segmentado', 'Promielocito', 'Restos celulares']

label2id = {name: i for i, name in enumerate(label_names)}
id2label = {i: name for i, name in enumerate(label_names)}

# Load image
img_path = "/lapix/valid/12345678_1_109_jpg.rf.be96518722a1ff0d45d4815947320a13.jpg"
image = Image.open(img_path).convert("RGB")

# Preprocess image
inputs = processor(images=image, return_tensors="pt")

# Run model (no gradients)
with torch.no_grad():
    outputs = model(**inputs)
    logits = outputs.logits

# Get predicted class index
predicted_idx = logits.argmax(-1).item()

# Map to label name
predicted_label = id2label[predicted_idx]

print(f"Predicted label: {predicted_label}")
import matplotlib.pyplot as plt

plt.imshow(image)
plt.title(f"Prediction: {predicted_label}")
plt.axis('off')
plt.show()

print("Labels mapping:")
for i, label in id2label.items():
    print(f"{i}: {label}")
from PIL import Image
import torch

model.eval()

def predict_image(image_path):
    img = Image.open(image_path)
    inputs = processor(images=img, return_tensors="pt").to(device)  # move to device if using GPU
    with torch.no_grad():
        outputs = model(**inputs)
    logits = outputs.logits
    pred_idx = logits.argmax(-1).item()
    return id2label[pred_idx]

image_path = "/lapix/valid/0102_1_014_jpg.rf.f101a9c1b90e91896cee095d7ff291ab.jpg"
print("Prediction:", predict_image(image_path))











